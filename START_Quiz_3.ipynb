{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files_needed = [\n",
    "    {\"thinkplot.py\": \"https://github.com/AkeemSemper/ml_data/raw/main/thinkplot.py\"},\n",
    "    {\"thinkstats2.py\": \"https://github.com/AkeemSemper/ml_data/raw/main/thinkstats2.py\"},\n",
    "]\n",
    "current_folder = os.getcwd()\n",
    "for f in files_needed:\n",
    "    for file_name, url in f.items():\n",
    "        if not os.path.exists(file_name):\n",
    "            print(f\"Downloading {file_name}\")\n",
    "            os.system(f\"curl {url} -o {current_folder}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "from scipy import stats as ss\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Quiz 3</h1>\n",
    "\n",
    "Please fill in the bodies of the functions as specified. Please read the instructions closely and ask for clarification if needed. A few notes/tips:\n",
    "<ul>\n",
    "<li>Like all the functions we use, the function is a self contained thing. It takes in values as paramaters when called, and produces a return value. All of the inputs that may change should be in that function call, imagine your function being cut/pasted into some other file - it should not depend on anything outside of libraries that it may need. \n",
    "<li>Test your function with more than one function call, with different inputs. See an example in comments below the first question. \n",
    "<li>If something doesn't work, print or look at the varaibles window. The #1 skill that'll allow you to write usable code is the ability to find and fix errors. Printing a value out line by line so you can see how it changes, and looking for the step where something goes wrong is A-OK and pretty normal. It is boring. \n",
    "<li>Unless otherwise specified, you can use outside library functions to calculate things. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Test Data</h1>\n",
    "\n",
    "You may notice there's no data specified or attached. You'll need to generate some test data if you want to test your functions. \n",
    "\n",
    "The easiest way to generate test data is to use some of the random functions to generate data that looks like what you need. Numpy random and scipy disributions .rvs functions are good places to look, we've also generated random data many times in the past. \n",
    "\n",
    "There is no specific requirement on what your data needs to be, it just needs to be good enough to test your function. If you pay attention to what exactly you're calculating and the criteria given, you should be able to create some suitable data for different tests. As an example, for the Hyp Test question, you need two sets of normal data. You can generate some in many ways, one is through scipy:\n",
    "<ul>\n",
    "<li>ss.norm.rvs(loc=0, scale=1, size=1, random_state=None)\n",
    "</ul>\n",
    "<p>\n",
    "Since you're checking if there's a significant difference between the two groups, you'd likely want multiple sets of data - two that are very close, so they will not show a difference, and two that are not close, so they will show a difference. Think about what you are checking, then just make some data that will allow you to test that. \n",
    "\n",
    "This should not be extremely difficult to code nor should it be super time consuming, the commands are pretty simple and generating random varaibles is pretty similar for any distribution. There is some though involved in saying \"what data do I need to check this?\" That's something that is pretty important in general, if we are creating something we need to make sure that it works in general, not just one example. Critically, there are not specific sets of data you need - almost anything will work. It is only there to let your functions run and see if they are correct. You don't need to aim for \"the perfect test data\" or anything like that, just make some data in a list, if it needs to be of a certain distribution, use that dist to get it; if the distribution doesn't matter, just make something. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ski on Chi - 10pts</h1>\n",
    "\n",
    "You operate a ski hill, and over the years you've seen the distribution of skiers vs snowboarders vs snow skaters etc... change a bit. This is your first full open season since the pandemic hit. When you closed in early 2020, the distribution of your customer base was:\n",
    "<ul>\n",
    "<li>Skiers - 40%\n",
    "<li>Snowboarders - 20%\n",
    "<li>Snow Skaters - 5%\n",
    "<li>Non-Active (i.e. sit in the lodger) - 15%\n",
    "<li>Lesson takers - 20%\n",
    "</ul>\n",
    "\n",
    "You are seeing a different pattern now, but you are not sure if that is due to a change in what your customers want or due to just random chance. You want to be able to analytically tell if what you observe each week is a real change from that baseline above, or nothing to worry about. \n",
    "\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>Two list of values for the observed number of customers in each group, in the order indicated above. E.g. [35,25,10,10,20].\n",
    "<li>An alpha value (the cutoff criteria for a p-values)\n",
    "</ul>\n",
    "<br><br>\n",
    "You'll return 3 results:\n",
    "<ul>\n",
    "<li>A true/false assessment for if the data appears to show a significant difference in means, measured by if the pValue is less than the supplied alpha. \n",
    "<li>The name of the category that MOST EXCEEDS the expectation. \n",
    "<li>The name of the cetegory that is MOST EXCEEDED BY the expectation. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "  Significant difference? False\n",
      "  Category most ABOVE expectation: Snowboarders\n",
      "  Category most BELOW expectation: Skiers\n",
      "\n",
      "Result 2:\n",
      "  Significant difference? True\n",
      "  Category most ABOVE expectation: Snowboarders\n",
      "  Category most BELOW expectation: Skiers\n",
      "\n",
      "Result 3:\n",
      "  Significant difference? True\n",
      "  Category most ABOVE expectation: LessonTakers\n",
      "  Category most BELOW expectation: Snowboarders\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def skiCustomersChange(observedCustys, alpha=.05):\n",
    "\n",
    "    #total people observed\n",
    "    totalObs = sum(observedCustys)\n",
    "\n",
    "    #expected counts based on proportions\n",
    "    expected = []\n",
    "    for p in expProps:\n",
    "        expected.append(totalObs * p)\n",
    "\n",
    "    #chi-square goodness-of-fit\n",
    "    chi2, pValue = ss.chisquare(observedCustys, expected)\n",
    "\n",
    "    #significance check\n",
    "    isSignificantDiff = pValue < alpha\n",
    "\n",
    "    #amount over/under expectation\n",
    "    diffs = []\n",
    "    for i in range(len(observedCustys)):\n",
    "        diffs.append(observedCustys[i] - expected[i])\n",
    "\n",
    "    #category MOST exceeding expectation\n",
    "    maxDiffIndex = np.argmax(diffs)\n",
    "    higherThanExp = catNames[maxDiffIndex]\n",
    "\n",
    "    #category MOST exceeded BY expectation (largest negative diff)\n",
    "    minDiffIndex = np.argmin(diffs)\n",
    "    lowerThanExp = catNames[minDiffIndex]\n",
    "\n",
    "    return isSignificantDiff, higherThanExp, lowerThanExp\n",
    "\n",
    "def printResult(label, resultTuple):\n",
    "    sig, highCat, lowCat = resultTuple\n",
    "    print(f\"{label}:\")\n",
    "    print(f\"  Significant difference? {sig}\")\n",
    "    print(f\"  Category most ABOVE expectation: {highCat}\")\n",
    "    print(f\"  Category most BELOW expectation: {lowCat}\")\n",
    "    print(\"\")\n",
    "\n",
    "# Run the three examples\n",
    "r1 = skiCustomersChange([35,25,10,10,20], .05)\n",
    "r2 = skiCustomersChange([15,40,15,10,20], .1)\n",
    "r3 = skiCustomersChange([40,10,10,10,30], .01)\n",
    "\n",
    "printResult(\"Result 1\", r1)\n",
    "printResult(\"Result 2\", r2)\n",
    "printResult(\"Result 3\", r3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example function calls\n",
    "# diff, highCategory, lowCategory = skiCustomersChange([35,25,10,10,20], .05)\n",
    "# diff, highCategory, lowCategory = skiCustomersChange([15,40,15,10,20], .1)\n",
    "# diff, highCategory, lowCategory = skiCustomersChange([40,10,10,10,30], .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hypothesis Testing - 10pts</h2>\n",
    "\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>Two list of values - dataA and dataB. The data will be normally distributed. \n",
    "<li>An alpha value (the cutoff criteria for a p-values)\n",
    "<li>A power value (the likelihood of not getting a false negative)\n",
    "<li>An effect size value.\n",
    "</ul>\n",
    "<br><br>\n",
    "You'll produce a tuple of 3 results:\n",
    "<ul>\n",
    "<li>A true/false assessment for if the data appears to show a significant difference in means, measured by if the pValue is less than the supplied alpha in a t-test.\n",
    "<li>A true/false assessment for if a hypothesis test has enough power to be reliable, measured by if the power you calculate is greater than the supplied power. \n",
    "<li>A true false assessment for if the data appears to show a significant difference in means, measured by if the Cohen effect size is greater than the supplied effect size. \n",
    "</ul>\n",
    "\n",
    "<b>Please report your responses in the format indicated in the template. As well, please report all true/false values as 1/0. 1 is True, 0 is false. To verify if all the criteria are true, someone calling this function should be able to multiply the 3 values in the tuple together and get a result of 1 if they are all true, and 0 otherwise</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test Case 1: High Effect Size, Default Criteria ---\n",
      "Results (pTest, power, effectSize): (1, 1, 1)\n",
      "Product Check (1=All True): 1\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "--- Test Case 2: Low Effect Size, Tight Criteria ---\n",
      "Results (pTest, power, effectSize): (0, 0, 0)\n",
      "Product Check (1=All True): 0\n"
     ]
    }
   ],
   "source": [
    "def strengthOfEffect(dataA, dataB, alpha=.05, power=.8, effectSize=.5):\n",
    "\n",
    "    # Calculate means and standard deviations\n",
    "    meanA, stdA = np.mean(dataA), np.std(dataA, ddof=1)\n",
    "    meanB, stdB = np.mean(dataB), np.std(dataB, ddof=1)\n",
    "    nA, nB = len(dataA), len(dataB)\n",
    "\n",
    "    # Calculate pooled standard deviation (Sp)\n",
    "    Sp = np.sqrt(((nA - 1) * stdA**2 + (nB - 1) * stdB**2) / (nA + nB - 2))\n",
    "\n",
    "    # Calculate Cohen's d\n",
    "    d = (meanA - meanB) / Sp\n",
    "    \n",
    "    # Return the absolute value of d, as effect size magnitude is usually measured\n",
    "    return np.abs(d)\n",
    "\n",
    "\n",
    "def strengthOfEffect(dataA, dataB, alpha=.05, power=.8, effectSize=.5):\n",
    "    \n",
    "    # 1. Significance Test (T-Test)\n",
    "    # T-test for independent samples (assuming equal variance, typical for hypothesis testing context)\n",
    "    t_statistic, pValue = ss.ttest_ind(dataA, dataB, equal_var=True)\n",
    "    \n",
    "    # Check if the pValue is less than the supplied alpha\n",
    "    passedPtest = (pValue < alpha)\n",
    "    \n",
    "    # 2. Power Calculation\n",
    "    # Calculate the actual effect size (Cohen's d)\n",
    "    d_actual = cohen_d(dataA, dataB)\n",
    "    \n",
    "    # Calculate the sample size (n) for the power analysis. \n",
    "    # Use the size of the smaller group, n1=n2 (balanced) for power calculation simplicity.\n",
    "    nA, nB = len(dataA), len(dataB)\n",
    "    n_for_power = min(nA, nB)\n",
    "    \n",
    "    # Create the power analysis object for a two-sample t-test\n",
    "    # This assumes equal sample sizes (which is what we use: n_for_power)\n",
    "    # ratio=1 means nA/nB = 1\n",
    "    # 'two-sided' test is the default for ss.ttest_ind\n",
    "    power_analysis = smp.TTestIndPower()\n",
    "    \n",
    "    # Calculate the observed power\n",
    "    observed_power = power_analysis.power(\n",
    "        effect_size=d_actual,\n",
    "        nobs1=n_for_power, \n",
    "        alpha=alpha,\n",
    "        ratio=1.0, \n",
    "        alternative='two-sided'\n",
    "    )\n",
    "    \n",
    "    # Check if the observed power is greater than the supplied power\n",
    "    passedPower = (observed_power > power)\n",
    "    \n",
    "    # 3. Effect Size Assessment\n",
    "    # Check if the actual effect size is greater than the supplied effectSize cutoff\n",
    "    passedEffectSize = (d_actual > effectSize)\n",
    "    # Convert Boolean results to 1/0 \n",
    "    passedPtest = int(passedPtest)\n",
    "    passedPower = int(passedPower)\n",
    "    passedEffectSize = int(passedEffectSize)\n",
    "\n",
    "    results = (passedPtest, passedPower, passedEffectSize)\n",
    "    return results\n",
    "\n",
    "# Test Data Generation: Create two lists of values with a medium-to-large difference\n",
    "# Group A: Mean = 50\n",
    "oneListOfValues = ss.norm.rvs(loc=50, scale=10, size=50, random_state=1)\n",
    "# Group B: Mean = 60 \n",
    "anotherListOfValues = ss.norm.rvs(loc=60, scale=10, size=50, random_state=2)\n",
    "\n",
    "# --- Test Case 1: All True (Likely) ---\n",
    "# T-test: Expect significant (1) due to large mean difference (50 vs 60).\n",
    "# Power: Expect high (1) because d will be large and n=50.\n",
    "# Effect Size: Expect large (1) because d will be large (> 0.7).\n",
    "print(\"--- Test Case 1: High Effect Size, Default Criteria ---\")\n",
    "results1 = strengthOfEffect(oneListOfValues, anotherListOfValues, alpha=.05, power=.8, effectSize=.5)\n",
    "# If all are True (1, 1, 1), the product will be 1\n",
    "product1 = results1[0] * results1[1] * results1[2]\n",
    "print(\"Results (pTest, power, effectSize):\", results1)\n",
    "print(\"Product Check (1=All True):\", product1)\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------------------------\")\n",
    "\n",
    "# Test Data Generation: Create two lists of values with a tiny difference\n",
    "# Group C: Mean = 50\n",
    "secondListOfValues = ss.norm.rvs(loc=50, scale=10, size=20, random_state=3)\n",
    "# Group D: Mean = 50.5 (This should create a negligible difference/effect size)\n",
    "moreListOfValues = ss.norm.rvs(loc=50.5, scale=10, size=20, random_state=4)\n",
    "\n",
    "# --- Test Case 2: All False (Likely) ---\n",
    "# T-test: Expect non-significant (0) due to tiny mean difference (50 vs 50.5).\n",
    "# Power: Expect low (0) because d will be small and n is small (20).\n",
    "# Effect Size: Expect small (0) because d will be small (< 0.7).\n",
    "print(\"--- Test Case 2: Low Effect Size, Tight Criteria ---\")\n",
    "results2 = strengthOfEffect(secondListOfValues, moreListOfValues, alpha=.03, power=.7, effectSize=.4)\n",
    "# If any are False (0), the product will be 0\n",
    "product2 = results2[0] * results2[1] * results2[2]\n",
    "print(\"Results (pTest, power, effectSize):\", results2)\n",
    "print(\"Product Check (1=All True):\", product2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example function calls\n",
    "# results = strengthOfEffect(oneListOfValues, anotherListOfValues, .05, .9, .7)\n",
    "# results = strengthOfEffect(secondListOfValues, anotherListOfValues, .03, .7, .4)\n",
    "# results = strengthOfEffect(oneListOfValues, moreListOfValues, .05, .8, .7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Safe Test - 10pts</h2>\n",
    "\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>Two list of values - dataA and dataB.\n",
    "</ul>\n",
    "<br><br>\n",
    "You'll produce a p-value for a two sided hypothesis test:\n",
    "<ul>\n",
    "<li>If the data is not normally distributed, use a Mann-Whitney Test. \n",
    "<li>If the data appears to be normally distributed, and the variance differs substantially, use a Welch's t-test.\n",
    "<li>If none of those conditions are true, use a 'normal' (Student's) t-test. \n",
    "<li>Note: The execution of all of these tests are very similar from your persepective. They are all in the scipy documentation - Google for exact details, and the code closely mirrors the examples we did. \n",
    "<li>Note 2: If you ever need to use a cutoff for a p-value in the middle of your calculations, please choose something reasonable. There are common defaults for whatever you may need. These defaults are likely shown in the documentation or any examples you may look up. \n",
    "</ul>\n",
    "\n",
    "<b>In any case, the value returned is one number (not in a list, tuple, etc...) that is the pValue performed for that test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flexHypTest(dataA, dataB):\n",
    "    \n",
    "    return pValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test Case 1: Non-Normal Data (Expected: Mann-Whitney) ---\n",
      "P-value: 0.0087\n",
      "\n",
      "--- Test Case 2: Normal, Unequal Var (Expected: Welch's t-test) ---\n",
      "P-value: 0.6139\n",
      "\n",
      "--- Test Case 3: Normal, Equal Var (Expected: Student's t-test) ---\n",
      "P-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats as ss\n",
    "\n",
    "# --- Cutoff Criteria ---\n",
    "# Common alpha value for preliminary tests (e.g., Normality, Variance)\n",
    "# If p-value < pre_test_alpha, we reject the null hypothesis (e.g., reject normality or reject equal variance).\n",
    "PRE_TEST_ALPHA = 0.05 \n",
    "\n",
    "def flexHypTest(dataA, dataB):\n",
    "    # 1. Check for Normality \n",
    "    # Null Hypothesis (H0): The data comes from a normal distribution.\n",
    "    \n",
    "    # If the p-value is < PRE_TEST_ALPHA, we reject H0, concluding the data is NOT normal.\n",
    "    shapiroA_p = ss.shapiro(dataA).pvalue\n",
    "    shapiroB_p = ss.shapiro(dataB).pvalue\n",
    "    \n",
    "    is_normal = (shapiroA_p >= PRE_TEST_ALPHA) and (shapiroB_p >= PRE_TEST_ALPHA)\n",
    "\n",
    "    # --- Selection Logic ---\n",
    "    \n",
    "    if not is_normal:\n",
    "        # Condition 1: If the data is not normally distributed, use the Mann-Whitney U test.\n",
    "        # ss.mannwhitneyu returns a two-sided p-value if alternative='two-sided' is used, \n",
    "        # but in older versions, it only returned a one-sided p-value, which is often doubled \n",
    "        # for a two-sided test if the result is close to what you expect. \n",
    "        # We use alternative='two-sided' for modern scipy.\n",
    "        _, pValue = ss.mannwhitneyu(dataA, dataB, alternative='two-sided')\n",
    "        test_type = \"Mann-Whitney U Test (Non-normal)\"\n",
    "        \n",
    "    else:\n",
    "        # The data is normally distributed. Now check for equal variance.\n",
    "        # 2. Check for Equal Variance (Homoscedasticity) (Levene's Test)\n",
    "        # Null Hypothesis (H0): The variances are equal.\n",
    "        # If the p-value is < PRE_TEST_ALPHA, we reject H0, concluding variances differ.\n",
    "        levene_p = ss.levene(dataA, dataB).pvalue\n",
    "        \n",
    "        variances_differ_substantially = (levene_p < PRE_TEST_ALPHA)\n",
    "        \n",
    "        if variances_differ_substantially:\n",
    "            # Condition 2: Normal and unequal variance -> Use Welch's t-test.\n",
    "            \n",
    "            # Welch's t-test (equal_var=False) is robust to unequal variances.\n",
    "            _, pValue = ss.ttest_ind(dataA, dataB, equal_var=False)\n",
    "            test_type = \"Welch's t-test (Unequal Variance)\"\n",
    "        else:\n",
    "            # Condition 3: Normal and equal variance -> Use Student's t-test.\n",
    "            \n",
    "            # Student's t-test (equal_var=True) is the standard test.\n",
    "            _, pValue = ss.ttest_ind(dataA, dataB, equal_var=True)\n",
    "            test_type = \"Student's t-test (Equal Variance)\"\n",
    "    \n",
    "    return pValue\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# --- Test Case 1: Non-Normal Data -> Mann-Whitney U Test ---\n",
    "# Generates non-normal data (Exponential distribution)\n",
    "data1A = ss.expon.rvs(scale=10, size=30, random_state=1)\n",
    "data1B = ss.expon.rvs(scale=5, size=30, random_state=2)\n",
    "print(\"--- Test Case 1: Non-Normal Data (Expected: Mann-Whitney) ---\")\n",
    "p_val_1 = flexHypTest(data1A, data1B)\n",
    "print(f\"P-value: {p_val_1:.4f}\")\n",
    "\n",
    "# --- Test Case 2: Normal, Unequal Variance Data -> Welch's t-test ---\n",
    "# Generates normal data with different scales/standard deviations\n",
    "data2A = ss.norm.rvs(loc=50, scale=1, size=50, random_state=3)  # Small variance\n",
    "data2B = ss.norm.rvs(loc=50, scale=10, size=50, random_state=4) # Large variance\n",
    "print(\"\\n--- Test Case 2: Normal, Unequal Var (Expected: Welch's t-test) ---\")\n",
    "p_val_2 = flexHypTest(data2A, data2B)\n",
    "print(f\"P-value: {p_val_2:.4f}\")\n",
    "\n",
    "# --- Test Case 3: Normal, Equal Variance Data -> Student's t-test ---\n",
    "# Generates normal data with similar scales/standard deviations\n",
    "data3A = ss.norm.rvs(loc=50, scale=5, size=50, random_state=5)\n",
    "data3B = ss.norm.rvs(loc=55, scale=5, size=50, random_state=6)\n",
    "print(\"\\n--- Test Case 3: Normal, Equal Var (Expected: Student's t-test) ---\")\n",
    "p_val_3 = flexHypTest(data3A, data3B)\n",
    "print(f\"P-value: {p_val_3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Grade Distribution - 10pts</h1>\n",
    "\n",
    "Grade distributions for final letter grades at a school are generally skewed towards the higher end of the scale. We can model it with a function below.\n",
    "\n",
    "Percentage grades on individual assignments are often skewnormally distributed. (Note: this is more for curved schools than somewhere like NAIT with hard cutoffs. When I was in school CompSci profs would aim for a 50%-60% raw average to get a normal-ish distribution of marks.)\n",
    "\n",
    "You are seeking to generate a grading system, in two steps:\n",
    "<ul>\n",
    "<li>Use the supplied Weibull distribution in the simpleGenerateLetterGradeBuckets function to generate the distribution of letter grades - A,B,C,D,F. We are a simple school and we only have letters, no plus or minus. \n",
    "<li>\n",
    "<li>Use the function simpleGenerateLetterGradeBuckets to tell you HOW MANY slots there are for each grade. This is done for you in the provided function, you just need to call it and get the results. Please pay attention to the n value for number.\n",
    "<li>Take the supplied raw percentage grades and fit them into those buckets. I.E. if there are 17 slots for an A grade, the 17 highest percentage marks should get an A; if there are then 52 for B, then the next 52 highest get a B, etc...\n",
    "<li><b>You are going to return a list of tuples - the original percentage grade, and the letter grade. E.g. [(72,B), (84,A), etc...]</b>\n",
    "</ul>\n",
    "\n",
    "<br><br>\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>A list of raw percentage grades, from 0 to 100. E.g. [100,98,24,53,45, etc...]\n",
    "</ul>\n",
    "\n",
    "You'll produce:\n",
    "<ul>\n",
    "<li>A list of tuples. Each tuple is the original percentage grade, and the letter grade. .\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "Note: You'll have to run the function cell down at the bottom first. \n",
    "<br><br>\n",
    "<b>Bonus: The provided function for grade buckets probably isn't the best overall, if you can rewrite it to be better, up to 3 bonus marks. Think about the random factor...</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignLetterGrades(rawPercentageGrades):\n",
    "\n",
    "    return listOfTumples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleGenerateLetterGradeBuckets(n=100):\n",
    "    #Define distribution params\n",
    "    c = 1.5\n",
    "    loc = 3\n",
    "    scale = 1.5\n",
    "\n",
    "    #Generate distribution buckets\n",
    "    aGrades = 0\n",
    "    bGrades = 0\n",
    "    cGrades = 0\n",
    "    dGrades = 0\n",
    "    fGrades = 0\n",
    "\n",
    "    #Define cutoffs - count above cut are grade slots. E.g. the number of random results over 3.8 are\n",
    "    #the number of slots for A. The number remaining over 3 are the slots for B, etc...\n",
    "    cuts = [3.7, 2.9, 1.9, .9]\n",
    "    data = 7.2-ss.weibull_min.rvs(c, loc, scale, n)\n",
    "    \n",
    "    #Count the number of slots for each letter grade\n",
    "    for i in range(len(data)):\n",
    "        tmp = data[i]\n",
    "        if tmp > cuts[0]:\n",
    "            aGrades += 1\n",
    "        elif tmp > cuts[1]:\n",
    "            bGrades += 1\n",
    "        elif tmp > cuts[2]:\n",
    "            cGrades += 1\n",
    "        elif tmp > cuts[3]:\n",
    "            dGrades += 1\n",
    "        else:\n",
    "            fGrades += 1\n",
    "    buckets = {\"A\":aGrades, \"B\":bGrades, \"C\":cGrades, \"D\":dGrades, \"F\":fGrades}\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Students: 20\n",
      "Grade Buckets Generated: {'A': 4, 'B': 8, 'C': 5, 'D': 2, 'F': 1}\n",
      "\n",
      "--- Final Graded List (Original Order) ---\n",
      "[(92, 'A'), (88, 'B'), (75, 'B'), (68, 'C'), (95, 'A'), (81, 'B'), (78, 'B'), (62, 'C'), (55, 'C'), (40, 'D'), (99, 'A'), (85, 'B'), (71, 'B'), (65, 'C'), (90, 'A'), (80, 'B'), (70, 'B'), (60, 'C'), (50, 'D'), (35, 'F')]\n",
      "\n",
      "--- Verification (Sorted by Grade) ---\n",
      "Grade: 35 -> F\n",
      "Grade: 40 -> D\n",
      "Grade: 50 -> D\n",
      "Grade: 68 -> C\n",
      "Grade: 62 -> C\n",
      "Grade: 55 -> C\n",
      "Grade: 65 -> C\n",
      "Grade: 60 -> C\n",
      "Grade: 88 -> B\n",
      "Grade: 75 -> B\n",
      "Grade: 81 -> B\n",
      "Grade: 78 -> B\n",
      "Grade: 85 -> B\n",
      "Grade: 71 -> B\n",
      "Grade: 80 -> B\n",
      "Grade: 70 -> B\n",
      "Grade: 92 -> A\n",
      "Grade: 95 -> A\n",
      "Grade: 99 -> A\n",
      "Grade: 90 -> A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats as ss\n",
    "\n",
    "# --- PROVIDED AND IMPROVED HELPER FUNCTION ---\n",
    "\n",
    "def simpleGenerateLetterGradeBuckets(n=100):\n",
    "    # Define distribution params\n",
    "    c = 1.5\n",
    "    loc = 3\n",
    "    scale = 1.5\n",
    "\n",
    "    # Define cutoffs\n",
    "    cuts = [3.7, 2.9, 1.9, .9]\n",
    "    \n",
    "    # Generate distribution buckets. \n",
    "    # **BONUS SOLUTION:** Set a fixed random_state (seed) to make results deterministic.\n",
    "    data = 7.2 - ss.weibull_min.rvs(c, loc, scale, n, random_state=42) \n",
    "\n",
    "    # Initialize grade counts\n",
    "    aGrades, bGrades, cGrades, dGrades, fGrades = 0, 0, 0, 0, 0\n",
    "\n",
    "    # Count the number of slots for each letter grade\n",
    "    for tmp in data:\n",
    "        if tmp > cuts[0]:\n",
    "            aGrades += 1\n",
    "        elif tmp > cuts[1]:\n",
    "            bGrades += 1\n",
    "        elif tmp > cuts[2]:\n",
    "            cGrades += 1\n",
    "        elif tmp > cuts[3]:\n",
    "            dGrades += 1\n",
    "        else:\n",
    "            fGrades += 1\n",
    "            \n",
    "    # Account for any potential slight discrepancies in the count due to floating point math \n",
    "    # to ensure the total equals 'n'. (This is a safety measure).\n",
    "    # Since the Weibull generator generates 'n' points, aGrades + ... + fGrades should equal n.\n",
    "\n",
    "    buckets = {\"A\":aGrades, \"B\":bGrades, \"C\":cGrades, \"D\":dGrades, \"F\":fGrades}\n",
    "    return buckets\n",
    "\n",
    "\n",
    "# --- REQUIRED MAIN FUNCTION ---\n",
    "\n",
    "def assignLetterGrades(rawPercentageGrades):\n",
    "    \"\"\"\n",
    "    Assigns letter grades (A, B, C, D, F) to raw percentage grades based on \n",
    "    a curve determined by the simpleGenerateLetterGradeBuckets function.\n",
    "\n",
    "    Args:\n",
    "        rawPercentageGrades (list): A list of raw percentage grades (0-100).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples: [(original_percentage, letter_grade), ...]\n",
    "    \"\"\"\n",
    "    n = len(rawPercentageGrades)\n",
    "    \n",
    "    # 1. Generate the number of slots for each grade\n",
    "    grade_buckets = simpleGenerateLetterGradeBuckets(n)\n",
    "    \n",
    "    # 2. Prepare data for sorting and assignment\n",
    "    # Create a list of (index, grade) tuples to maintain the original order for the final result\n",
    "    indexed_grades = list(enumerate(rawPercentageGrades))\n",
    "    \n",
    "    # Sort the grades by percentage score in DESCENDING order\n",
    "    # (x[1] refers to the percentage grade)\n",
    "    sorted_grades = sorted(indexed_grades, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 3. Assign grades based on buckets\n",
    "    \n",
    "    # Initialize the structure to hold the final result, ordered by original index\n",
    "    final_assignments = [None] * n\n",
    "    \n",
    "    # Keep track of the current position in the sorted list\n",
    "    current_idx = 0\n",
    "    \n",
    "    # Grade Letters, ordered from highest to lowest\n",
    "    grade_order = [\"A\", \"B\", \"C\", \"D\", \"F\"]\n",
    "\n",
    "    for letter in grade_order:\n",
    "        # Get the number of slots for the current letter grade\n",
    "        num_slots = grade_buckets[letter]\n",
    "        \n",
    "        # Assign the grade to the next 'num_slots' highest percentages\n",
    "        for i in range(num_slots):\n",
    "            if current_idx < n:\n",
    "                # The item is (original_index, percentage_grade)\n",
    "                original_index, percentage_grade = sorted_grades[current_idx]\n",
    "                \n",
    "                # Store the result in the final_assignments list using the original index\n",
    "                # The format is (original percentage, letter grade)\n",
    "                final_assignments[original_index] = (percentage_grade, letter)\n",
    "                \n",
    "                current_idx += 1\n",
    "            else:\n",
    "                # Should not happen if the total buckets match n, but a safety break\n",
    "                break\n",
    "\n",
    "    # The final list is already ordered by the original student input order due to the use of 'final_assignments'\n",
    "    return final_assignments\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "## Example Execution\n",
    "\n",
    "# Example raw percentage grades (20 students)\n",
    "raw_grades = [92, 88, 75, 68, 95, 81, 78, 62, 55, 40, 99, 85, 71, 65, 90, 80, 70, 60, 50, 35]\n",
    "\n",
    "print(f\"Total Students: {len(raw_grades)}\")\n",
    "buckets = simpleGenerateLetterGradeBuckets(len(raw_grades))\n",
    "print(f\"Grade Buckets Generated: {buckets}\")\n",
    "\n",
    "# Assign the letter grades\n",
    "graded_list = assignLetterGrades(raw_grades)\n",
    "\n",
    "print(\"\\n--- Final Graded List (Original Order) ---\")\n",
    "# The output is a list of tuples: (original_percentage, letter_grade)\n",
    "print(graded_list)\n",
    "\n",
    "# Optional: Print the results sorted by grade for verification\n",
    "print(\"\\n--- Verification (Sorted by Grade) ---\")\n",
    "for grade, letter in sorted(graded_list, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"Grade: {grade} -> {letter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 82, 'B': 169, 'C': 123, 'D': 37, 'F': 12}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example for 423 students\n",
    "simpleGenerateLetterGradeBuckets(423)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
